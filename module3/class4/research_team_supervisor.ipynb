{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langgraph langchain-google-genai langchain-community duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") \n",
    "print(f\"API Key loaded: {API_KEY[:10]}...\" if API_KEY else \"No API Key found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_langgraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state_definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our research team\n",
    "class ResearchState(TypedDict):\n",
    "    \"\"\"The state of the research team.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    user_query: str  # Original user question\n",
    "    search_results: str  # Results from searcher\n",
    "    draft: str  # Draft from writer\n",
    "    critique: str  # Feedback from critic\n",
    "    final_answer: str  # Final compiled answer\n",
    "    next_agent: str  # Which agent to call next\n",
    "    iteration_count: int  # Track iterations to prevent infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"LLM initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supervisor_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor Node - Orchestrates the research team\n",
    "def supervisor_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    The supervisor reviews the current state and decides which agent to call next.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    search_results = state.get(\"search_results\", \"\")\n",
    "    draft = state.get(\"draft\", \"\")\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    iteration = state.get(\"iteration_count\", 0)\n",
    "    \n",
    "    # Create supervisor prompt\n",
    "    supervisor_prompt = f\"\"\"You are a research supervisor coordinating a team of agents.\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Current State:\n",
    "- Search Results: {\"Available\" if search_results else \"Not yet gathered\"}\n",
    "- Draft: {\"Available\" if draft else \"Not yet written\"}\n",
    "- Critique: {\"Available\" if critique else \"Not yet reviewed\"}\n",
    "- Iteration: {iteration}\n",
    "\n",
    "Your team consists of:\n",
    "1. Searcher - Finds relevant information and data\n",
    "2. Writer - Synthesizes information into a coherent draft\n",
    "3. Critic - Reviews drafts for accuracy and completeness\n",
    "\n",
    "Based on the current state, decide the next step:\n",
    "- If no search results exist, call \"Searcher\"\n",
    "- If search results exist but no draft, call \"Writer\"\n",
    "- If draft exists but no critique, call \"Critic\"\n",
    "- If critique suggests improvements and iteration < 2, call \"Writer\" for revision\n",
    "- If work is complete or iteration >= 2, call \"Finish\"\n",
    "\n",
    "Respond with ONLY ONE WORD: Searcher, Writer, Critic, or Finish\"\"\"\n",
    "    \n",
    "    response = llm.invoke(supervisor_prompt)\n",
    "    decision = response.content.strip()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ SUPERVISOR DECISION: {decision}\")\n",
    "    \n",
    "    return {\n",
    "        \"next_agent\": decision,\n",
    "        \"iteration_count\": iteration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searcher_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searcher Agent Node\n",
    "def searcher_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Searches for relevant information using web search (simulated).\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    \n",
    "    print(\"\\nðŸ” SEARCHER AGENT: Gathering information...\")\n",
    "    \n",
    "    # Create search prompt for LLM to simulate search results\n",
    "    search_prompt = f\"\"\"You are a research searcher agent. Your job is to find and summarize \n",
    "relevant information about the following query:\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Provide a comprehensive summary of key facts, statistics, pros, cons, and relevant \n",
    "information that would help answer this query. Format your response as if you've \n",
    "gathered information from multiple authoritative sources.\n",
    "\n",
    "Include:\n",
    "- Key facts and statistics\n",
    "- Different perspectives\n",
    "- Recent developments\n",
    "- Expert opinions\"\"\"\n",
    "    \n",
    "    response = llm.invoke(search_prompt)\n",
    "    search_results = response.content\n",
    "    \n",
    "    print(f\"âœ… Search completed: {len(search_results)} characters of data gathered\")\n",
    "    \n",
    "    return {\n",
    "        \"search_results\": search_results,\n",
    "        \"messages\": [AIMessage(content=f\"Search completed for: {user_query}\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "writer_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer Agent Node\n",
    "def writer_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Synthesizes search results into a coherent draft answer.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    search_results = state.get(\"search_results\", \"\")\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    previous_draft = state.get(\"draft\", \"\")\n",
    "    \n",
    "    print(\"\\nâœï¸ WRITER AGENT: Crafting response...\")\n",
    "    \n",
    "    # Build the revision section if there's a critique\n",
    "    revision_section = \"\"\n",
    "    if critique:\n",
    "        revision_section = f\"\\nPrevious Draft:\\n{previous_draft}\\n\\nCritique to Address:\\n{critique}\\n\"\n",
    "    \n",
    "    # Build the critique instruction\n",
    "    critique_instruction = \"- Addresses the critique points\" if critique else \"\"\n",
    "    \n",
    "    # Create writer prompt\n",
    "    writer_prompt = f\"\"\"You are a research writer agent. Your job is to synthesize \n",
    "information into a clear, well-structured answer.\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "{revision_section}\n",
    "Write a comprehensive, well-structured answer that:\n",
    "- Directly addresses the user's question\n",
    "- Presents information clearly and logically\n",
    "- Includes relevant facts and perspectives\n",
    "- Is balanced and objective\n",
    "{critique_instruction}\n",
    "\n",
    "Provide your draft answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(writer_prompt)\n",
    "    draft = response.content\n",
    "    \n",
    "    print(f\"âœ… Draft completed: {len(draft)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"draft\": draft,\n",
    "        \"messages\": [AIMessage(content=\"Draft answer created\")],\n",
    "        \"iteration_count\": state.get(\"iteration_count\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critic_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Agent Node\n",
    "def critic_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Reviews the draft for accuracy, completeness, and clarity.\n",
    "    \"\"\"\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    draft = state.get(\"draft\", \"\")\n",
    "    search_results = state.get(\"search_results\", \"\")\n",
    "    \n",
    "    print(\"\\nðŸ”Ž CRITIC AGENT: Reviewing draft...\")\n",
    "    \n",
    "    # Create critic prompt\n",
    "    critic_prompt = f\"\"\"You are a research critic agent. Your job is to review drafts \n",
    "for accuracy, completeness, and clarity.\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Draft Answer:\n",
    "{draft}\n",
    "\n",
    "Available Research:\n",
    "{search_results[:500]}...\n",
    "\n",
    "Evaluate the draft and provide:\n",
    "1. What's good about it\n",
    "2. What's missing or could be improved\n",
    "3. Any inaccuracies or unclear points\n",
    "4. Overall assessment: APPROVED or NEEDS_REVISION\n",
    "\n",
    "Be constructive and specific in your feedback.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(critic_prompt)\n",
    "    critique = response.content\n",
    "    \n",
    "    print(f\"âœ… Critique completed\")\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique,\n",
    "        \"messages\": [AIMessage(content=\"Draft reviewed by critic\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "router_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router function for conditional edges\n",
    "def route_after_supervisor(state: ResearchState) -> Literal[\"searcher\", \"writer\", \"critic\", \"finish\"]:\n",
    "    \"\"\"\n",
    "    Routes to the appropriate agent based on supervisor's decision.\n",
    "    \"\"\"\n",
    "    next_agent = state.get(\"next_agent\", \"\").lower()\n",
    "    \n",
    "    if \"searcher\" in next_agent:\n",
    "        return \"searcher\"\n",
    "    elif \"writer\" in next_agent:\n",
    "        return \"writer\"\n",
    "    elif \"critic\" in next_agent:\n",
    "        return \"critic\"\n",
    "    else:\n",
    "        return \"finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finalize_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize node - prepares the final answer\n",
    "def finalize_answer(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Prepares the final answer for the user.\n",
    "    \"\"\"\n",
    "    draft = state.get(\"draft\", \"\")\n",
    "    critique = state.get(\"critique\", \"\")\n",
    "    \n",
    "    print(\"\\nâœ¨ FINALIZING ANSWER...\")\n",
    "    \n",
    "    final_answer = f\"\"\"# Research Team Final Answer\n",
    "\n",
    "{draft}\n",
    "\n",
    "---\n",
    "Quality Review: {\"Approved by critic\" if critique else \"Completed\"}\n",
    "\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": final_answer,\n",
    "        \"messages\": [AIMessage(content=\"Research complete!\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the research team graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add all nodes\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "workflow.add_node(\"searcher\", searcher_agent)\n",
    "workflow.add_node(\"writer\", writer_agent)\n",
    "workflow.add_node(\"critic\", critic_agent)\n",
    "workflow.add_node(\"finalize\", finalize_answer)\n",
    "\n",
    "# Set entry point\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# Add conditional edges from supervisor to agents\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_after_supervisor,\n",
    "    {\n",
    "        \"searcher\": \"searcher\",\n",
    "        \"writer\": \"writer\",\n",
    "        \"critic\": \"critic\",\n",
    "        \"finish\": \"finalize\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# All worker agents route back to supervisor\n",
    "workflow.add_edge(\"searcher\", \"supervisor\")\n",
    "workflow.add_edge(\"writer\", \"supervisor\")\n",
    "workflow.add_edge(\"critic\", \"supervisor\")\n",
    "\n",
    "# Finalize goes to END\n",
    "workflow.add_edge(\"finalize\", END)\n",
    "\n",
    "# Compile the graph\n",
    "research_graph = workflow.compile()\n",
    "\n",
    "print(\"âœ… Research Team Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(research_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation",
   "metadata": {},
   "source": [
    "## How the Research Team Works\n",
    "\n",
    "### Architecture\n",
    "1. **Supervisor Node**: Orchestrates the workflow by deciding which agent to call next\n",
    "2. **Searcher Agent**: Gathers relevant information and data\n",
    "3. **Writer Agent**: Synthesizes information into a coherent draft\n",
    "4. **Critic Agent**: Reviews drafts for quality and completeness\n",
    "5. **Finalize Node**: Prepares the final answer\n",
    "\n",
    "### Workflow\n",
    "1. User submits a complex research question\n",
    "2. Supervisor analyzes the state and routes to Searcher\n",
    "3. Searcher gathers information â†’ back to Supervisor\n",
    "4. Supervisor routes to Writer\n",
    "5. Writer creates draft â†’ back to Supervisor\n",
    "6. Supervisor routes to Critic\n",
    "7. Critic reviews draft â†’ back to Supervisor\n",
    "8. Supervisor decides: revise (back to Writer) or finish\n",
    "9. Finalize prepares the final answer\n",
    "\n",
    "### Key Features\n",
    "- **Conditional Routing**: Supervisor uses LLM to make intelligent routing decisions\n",
    "- **Iterative Refinement**: Can loop back for revisions based on critique\n",
    "- **State Management**: Tracks all intermediate results and progress\n",
    "- **Loop Prevention**: Iteration counter prevents infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Complex research question\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 1: Nuclear Fusion vs Solar Power\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"user_query\": \"What are the pros and cons of nuclear fusion vs. solar power for base-load energy by 2050?\",\n",
    "    \"search_results\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"final_answer\": \"\",\n",
    "    \"next_agent\": \"\",\n",
    "    \"iteration_count\": 0\n",
    "}\n",
    "\n",
    "result = research_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Another complex question\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: AI Impact on Employment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "initial_state_2 = {\n",
    "    \"messages\": [],\n",
    "    \"user_query\": \"How will artificial intelligence impact employment in the healthcare sector over the next decade?\",\n",
    "    \"search_results\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"final_answer\": \"\",\n",
    "    \"next_agent\": \"\",\n",
    "    \"iteration_count\": 0\n",
    "}\n",
    "\n",
    "result_2 = research_graph.invoke(initial_state_2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result_2[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: Ask your own research question\n",
    "def ask_research_team(question: str):\n",
    "    \"\"\"\n",
    "    Helper function to ask the research team a question.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"RESEARCH QUESTION: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    state = {\n",
    "        \"messages\": [],\n",
    "        \"user_query\": question,\n",
    "        \"search_results\": \"\",\n",
    "        \"draft\": \"\",\n",
    "        \"critique\": \"\",\n",
    "        \"final_answer\": \"\",\n",
    "        \"next_agent\": \"\",\n",
    "        \"iteration_count\": 0\n",
    "    }\n",
    "    \n",
    "    result = research_graph.invoke(state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL ANSWER:\")\n",
    "    print(\"=\"*70)\n",
    "    print(result[\"final_answer\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# ask_research_team(\"What are the environmental impacts of cryptocurrency mining?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
